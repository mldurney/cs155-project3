{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, CuDNNLSTM, LSTM, Lambda\n",
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "import numpy as np\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code used liberally from https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "sonnets = []\n",
    "with open(\"../data/shakespeare.txt\") as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        # Flag start of sonnet, read in next 14 lines\n",
    "        if any(char.isdigit() for char in line):\n",
    "            curr_sonnet = \"\"\n",
    "            for i in range(14):\n",
    "                curr_sonnet += f.readline().strip().lower()\n",
    "                curr_sonnet += \"\\n\" if i != 13 else \"\"\n",
    "            sonnets.append(curr_sonnet)\n",
    "        line = f.readline()\n",
    "        \n",
    "# Vectorization prep\n",
    "chars = sorted(list(set(\"\".join(sonnets))))\n",
    "char_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# Read subsequences from each sonnet, add to training list\n",
    "# Don't read across sonnets?\n",
    "length = 40\n",
    "step = 1\n",
    "tr_data = []\n",
    "tar_char = []\n",
    "for s in sonnets:\n",
    "    for i in range(0, len(s) - length, step):\n",
    "        tr_data.append(s[i:i+length])\n",
    "        tar_char.append(s[i+length])\n",
    "        \n",
    "tr_data_full = []\n",
    "tar_char_full = []\n",
    "sonnets_full = \"\\n\".join(sonnets)\n",
    "for i in range(0, len(sonnets_full) - length, step):\n",
    "    tr_data_full.append(sonnets_full[i:i+length])\n",
    "    tar_char_full.append(sonnets_full[i+length])\n",
    "    \n",
    "# Vectorize training data\n",
    "X = np.zeros((len(tr_data), length, len(chars)), dtype=np.bool)\n",
    "Y = np.zeros((len(tr_data), len(chars)), dtype=np.bool)\n",
    "\n",
    "X_full = np.zeros((len(tr_data_full), length, len(chars)), dtype=np.bool)\n",
    "Y_full = np.zeros((len(tr_data_full), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, seq in enumerate(tr_data):\n",
    "    for j, char in enumerate(seq):\n",
    "        X[i, j, char_index[char]] = 1\n",
    "    Y[i, char_index[tar_char[i]]] = 1\n",
    "    \n",
    "for i, seq in enumerate(tr_data_full):\n",
    "    for j, char in enumerate(seq):\n",
    "        X_full[i, j, char_index[char]] = 1\n",
    "    Y_full[i, char_index[tar_char_full[i]]] = 1\n",
    "    \n",
    "X_full_shuff, Y_full_shuff = shuffle(X_full, Y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               85504     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                4902      \n",
      "=================================================================\n",
      "Total params: 90,406\n",
      "Trainable params: 90,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(length, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78584 samples, validate on 8732 samples\n",
      "Epoch 1/10\n",
      "78584/78584 [==============================] - 36s 463us/step - loss: 2.1044 - val_loss: 2.0401\n",
      "Epoch 2/10\n",
      "78584/78584 [==============================] - 36s 455us/step - loss: 1.9595 - val_loss: 1.9569\n",
      "Epoch 3/10\n",
      "78584/78584 [==============================] - 35s 446us/step - loss: 1.8694 - val_loss: 1.8935\n",
      "Epoch 4/10\n",
      "78584/78584 [==============================] - 35s 443us/step - loss: 1.8000 - val_loss: 1.8544\n",
      "Epoch 5/10\n",
      "78584/78584 [==============================] - 34s 439us/step - loss: 1.7452 - val_loss: 1.8207\n",
      "Epoch 6/10\n",
      "78584/78584 [==============================] - 35s 447us/step - loss: 1.7002 - val_loss: 1.7980\n",
      "Epoch 7/10\n",
      "78584/78584 [==============================] - 36s 453us/step - loss: 1.6588 - val_loss: 1.7753\n",
      "Epoch 8/10\n",
      "78584/78584 [==============================] - 35s 442us/step - loss: 1.6247 - val_loss: 1.7579\n",
      "Epoch 9/10\n",
      "78584/78584 [==============================] - 35s 439us/step - loss: 1.5923 - val_loss: 1.7481\n",
      "Epoch 10/10\n",
      "78584/78584 [==============================] - 35s 448us/step - loss: 1.5627 - val_loss: 1.7378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f43c7cffa20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=10, validation_split=0.1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               85504     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                4902      \n",
      "=================================================================\n",
      "Total params: 90,406\n",
      "Trainable params: 90,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "amodel = Sequential()\n",
    "amodel.add(LSTM(128, input_shape=(length, len(chars))))\n",
    "amodel.add(Dense(len(chars), activation='softmax'))\n",
    "amodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "amodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78584 samples, validate on 8732 samples\n",
      "Epoch 1/10\n",
      "78584/78584 [==============================] - 36s 455us/step - loss: 2.6588 - val_loss: 2.2856\n",
      "Epoch 2/10\n",
      "78584/78584 [==============================] - 35s 449us/step - loss: 2.1919 - val_loss: 2.1182\n",
      "Epoch 3/10\n",
      "78584/78584 [==============================] - 35s 440us/step - loss: 2.0526 - val_loss: 2.0236\n",
      "Epoch 4/10\n",
      "78584/78584 [==============================] - 34s 438us/step - loss: 1.9471 - val_loss: 1.9341\n",
      "Epoch 5/10\n",
      "78584/78584 [==============================] - 35s 448us/step - loss: 1.8607 - val_loss: 1.8848\n",
      "Epoch 6/10\n",
      "78584/78584 [==============================] - 35s 443us/step - loss: 1.8021 - val_loss: 1.8509\n",
      "Epoch 7/10\n",
      "78584/78584 [==============================] - 35s 446us/step - loss: 1.7567 - val_loss: 1.8143\n",
      "Epoch 8/10\n",
      "78584/78584 [==============================] - 34s 439us/step - loss: 1.7151 - val_loss: 1.7918\n",
      "Epoch 9/10\n",
      "78584/78584 [==============================] - 35s 445us/step - loss: 1.6817 - val_loss: 1.7837\n",
      "Epoch 10/10\n",
      "78584/78584 [==============================] - 35s 444us/step - loss: 1.6512 - val_loss: 1.7587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4385c89e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amodel.fit(X, Y, epochs=10, validation_split=0.1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               85504     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 38)                4902      \n",
      "=================================================================\n",
      "Total params: 90,406\n",
      "Trainable params: 90,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "namodel = Sequential()\n",
    "namodel.add(LSTM(128, input_shape=(length, len(chars))))\n",
    "namodel.add(Dense(len(chars), activation='softmax'))\n",
    "namodel.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\")\n",
    "namodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78584 samples, validate on 8732 samples\n",
      "Epoch 1/10\n",
      "78584/78584 [==============================] - 36s 453us/step - loss: 2.4465 - val_loss: 2.1392\n",
      "Epoch 2/10\n",
      "78584/78584 [==============================] - 35s 448us/step - loss: 2.0254 - val_loss: 1.9742\n",
      "Epoch 3/10\n",
      "78584/78584 [==============================] - 35s 448us/step - loss: 1.8815 - val_loss: 1.8643\n",
      "Epoch 4/10\n",
      "78584/78584 [==============================] - 35s 449us/step - loss: 1.7878 - val_loss: 1.8172\n",
      "Epoch 5/10\n",
      "78584/78584 [==============================] - 35s 445us/step - loss: 1.7117 - val_loss: 1.7897\n",
      "Epoch 6/10\n",
      "78584/78584 [==============================] - 35s 441us/step - loss: 1.6620 - val_loss: 1.7498\n",
      "Epoch 7/10\n",
      "78584/78584 [==============================] - 36s 461us/step - loss: 1.6208 - val_loss: 1.7397\n",
      "Epoch 8/10\n",
      "78584/78584 [==============================] - 35s 444us/step - loss: 1.5828 - val_loss: 1.7214\n",
      "Epoch 9/10\n",
      "78584/78584 [==============================] - 35s 450us/step - loss: 1.5500 - val_loss: 1.7215\n",
      "Epoch 10/10\n",
      "78584/78584 [==============================] - 35s 445us/step - loss: 1.5189 - val_loss: 1.7055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4385b57cf8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namodel.fit(X, Y, epochs=10, validation_split=0.1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 128)               86016     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 38)                4902      \n",
      "=================================================================\n",
      "Total params: 90,918\n",
      "Trainable params: 90,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bmodel = Sequential()\n",
    "bmodel.add(CuDNNLSTM(128, input_shape=(length, len(chars))))\n",
    "bmodel.add(Dense(len(chars), activation='softmax'))\n",
    "bmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "bmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78584 samples, validate on 8732 samples\n",
      "Epoch 1/10\n",
      "78584/78584 [==============================] - 9s 114us/step - loss: 2.6415 - val_loss: 2.3185\n",
      "Epoch 2/10\n",
      "78584/78584 [==============================] - 8s 96us/step - loss: 2.2153 - val_loss: 2.1285\n",
      "Epoch 3/10\n",
      "78584/78584 [==============================] - 8s 97us/step - loss: 2.0599 - val_loss: 2.0311\n",
      "Epoch 4/10\n",
      "78584/78584 [==============================] - 8s 96us/step - loss: 1.9599 - val_loss: 1.9648\n",
      "Epoch 5/10\n",
      "78584/78584 [==============================] - 8s 96us/step - loss: 1.8878 - val_loss: 1.9188\n",
      "Epoch 6/10\n",
      "78584/78584 [==============================] - 7s 93us/step - loss: 1.8288 - val_loss: 1.8824\n",
      "Epoch 7/10\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.7811 - val_loss: 1.8456\n",
      "Epoch 8/10\n",
      "78584/78584 [==============================] - 7s 93us/step - loss: 1.7386 - val_loss: 1.8346\n",
      "Epoch 9/10\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.7019 - val_loss: 1.8137\n",
      "Epoch 10/10\n",
      "78584/78584 [==============================] - 7s 94us/step - loss: 1.6678 - val_loss: 1.7959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f438427ddd8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmodel.fit(X, Y, epochs=10, validation_split=0.1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78584 samples, validate on 8732 samples\n",
      "Epoch 11/100\n",
      "78584/78584 [==============================] - 8s 96us/step - loss: 1.6374 - val_loss: 1.7826\n",
      "Epoch 12/100\n",
      "78584/78584 [==============================] - 7s 93us/step - loss: 1.6082 - val_loss: 1.7711\n",
      "Epoch 13/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.5820 - val_loss: 1.7568\n",
      "Epoch 14/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.5565 - val_loss: 1.7596\n",
      "Epoch 15/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.5322 - val_loss: 1.7594\n",
      "Epoch 16/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.5087 - val_loss: 1.7599\n",
      "Epoch 17/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.4861 - val_loss: 1.7566\n",
      "Epoch 18/100\n",
      "78584/78584 [==============================] - 7s 94us/step - loss: 1.4644 - val_loss: 1.7670\n",
      "Epoch 19/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.4426 - val_loss: 1.7636\n",
      "Epoch 20/100\n",
      "78584/78584 [==============================] - 7s 91us/step - loss: 1.4215 - val_loss: 1.7777\n",
      "Epoch 21/100\n",
      "78584/78584 [==============================] - 7s 91us/step - loss: 1.4006 - val_loss: 1.7753\n",
      "Epoch 22/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.3797 - val_loss: 1.7902\n",
      "Epoch 23/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.3584 - val_loss: 1.8042\n",
      "Epoch 24/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.3386 - val_loss: 1.8113\n",
      "Epoch 25/100\n",
      "78584/78584 [==============================] - 7s 91us/step - loss: 1.3187 - val_loss: 1.8112\n",
      "Epoch 26/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.2989 - val_loss: 1.8350\n",
      "Epoch 27/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.2786 - val_loss: 1.8566\n",
      "Epoch 28/100\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 1.2588 - val_loss: 1.8680\n",
      "Epoch 29/100\n",
      "29568/78584 [==========>...................] - ETA: 4s - loss: 1.2185"
     ]
    }
   ],
   "source": [
    "bmodel.fit(X, Y, epochs=100, validation_split=0.1, batch_size=128, initial_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 78584 samples, validate on 8732 samples\n",
      "Epoch 101/200\n",
      "78584/78584 [==============================] - 7s 92us/step - loss: 0.6705 - val_loss: 2.9944\n",
      "Epoch 102/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6682 - val_loss: 2.9917\n",
      "Epoch 103/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6678 - val_loss: 3.0015\n",
      "Epoch 104/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6671 - val_loss: 3.0111\n",
      "Epoch 105/200\n",
      "78584/78584 [==============================] - 7s 90us/step - loss: 0.6615 - val_loss: 3.0075\n",
      "Epoch 106/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6592 - val_loss: 3.0302\n",
      "Epoch 107/200\n",
      "78584/78584 [==============================] - 7s 91us/step - loss: 0.6612 - val_loss: 3.0431\n",
      "Epoch 108/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6549 - val_loss: 3.0351\n",
      "Epoch 109/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6553 - val_loss: 3.0538\n",
      "Epoch 110/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6538 - val_loss: 3.0259\n",
      "Epoch 111/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6510 - val_loss: 3.0688\n",
      "Epoch 112/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6525 - val_loss: 3.0413\n",
      "Epoch 113/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6498 - val_loss: 3.0772\n",
      "Epoch 114/200\n",
      "78584/78584 [==============================] - 7s 90us/step - loss: 0.6477 - val_loss: 3.0771\n",
      "Epoch 115/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6449 - val_loss: 3.0743\n",
      "Epoch 116/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6434 - val_loss: 3.0888\n",
      "Epoch 117/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6425 - val_loss: 3.0957\n",
      "Epoch 118/200\n",
      "78584/78584 [==============================] - 7s 90us/step - loss: 0.6408 - val_loss: 3.0879\n",
      "Epoch 119/200\n",
      "78584/78584 [==============================] - 7s 90us/step - loss: 0.6405 - val_loss: 3.1104\n",
      "Epoch 120/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6383 - val_loss: 3.1098\n",
      "Epoch 121/200\n",
      "78584/78584 [==============================] - 7s 90us/step - loss: 0.6349 - val_loss: 3.1244\n",
      "Epoch 122/200\n",
      "78584/78584 [==============================] - 7s 90us/step - loss: 0.6355 - val_loss: 3.1345\n",
      "Epoch 123/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6373 - val_loss: 3.1302\n",
      "Epoch 124/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6311 - val_loss: 3.1483\n",
      "Epoch 125/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6301 - val_loss: 3.1699\n",
      "Epoch 126/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6297 - val_loss: 3.1788\n",
      "Epoch 127/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6325 - val_loss: 3.1554\n",
      "Epoch 128/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6287 - val_loss: 3.1467\n",
      "Epoch 129/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6233 - val_loss: 3.1428\n",
      "Epoch 130/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6228 - val_loss: 3.1863\n",
      "Epoch 131/200\n",
      "78584/78584 [==============================] - 7s 90us/step - loss: 0.6236 - val_loss: 3.1861\n",
      "Epoch 132/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6241 - val_loss: 3.1897\n",
      "Epoch 133/200\n",
      "78584/78584 [==============================] - 7s 90us/step - loss: 0.6231 - val_loss: 3.1652\n",
      "Epoch 134/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6213 - val_loss: 3.1976\n",
      "Epoch 135/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6184 - val_loss: 3.1981\n",
      "Epoch 136/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6193 - val_loss: 3.2159\n",
      "Epoch 137/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6222 - val_loss: 3.2210\n",
      "Epoch 138/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6182 - val_loss: 3.1948\n",
      "Epoch 139/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6182 - val_loss: 3.2157\n",
      "Epoch 140/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6183 - val_loss: 3.2163\n",
      "Epoch 141/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6162 - val_loss: 3.2150\n",
      "Epoch 142/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6135 - val_loss: 3.2186\n",
      "Epoch 143/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6110 - val_loss: 3.2404\n",
      "Epoch 144/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6127 - val_loss: 3.2305\n",
      "Epoch 145/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6131 - val_loss: 3.2546\n",
      "Epoch 146/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6061 - val_loss: 3.2665\n",
      "Epoch 147/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6058 - val_loss: 3.2287\n",
      "Epoch 148/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6049 - val_loss: 3.2394\n",
      "Epoch 149/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6045 - val_loss: 3.2506\n",
      "Epoch 150/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6038 - val_loss: 3.2607\n",
      "Epoch 151/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6030 - val_loss: 3.2806\n",
      "Epoch 152/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6008 - val_loss: 3.2856\n",
      "Epoch 153/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6025 - val_loss: 3.2651\n",
      "Epoch 154/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5999 - val_loss: 3.2662\n",
      "Epoch 155/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5980 - val_loss: 3.2778\n",
      "Epoch 156/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6005 - val_loss: 3.3030\n",
      "Epoch 157/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.6038 - val_loss: 3.2808\n",
      "Epoch 158/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.6013 - val_loss: 3.3060\n",
      "Epoch 159/200\n",
      "78584/78584 [==============================] - 7s 91us/step - loss: 0.5973 - val_loss: 3.3046\n",
      "Epoch 160/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5945 - val_loss: 3.2966\n",
      "Epoch 161/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5948 - val_loss: 3.3156\n",
      "Epoch 162/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5987 - val_loss: 3.3220\n",
      "Epoch 163/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5932 - val_loss: 3.3154\n",
      "Epoch 164/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5926 - val_loss: 3.3272\n",
      "Epoch 165/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5961 - val_loss: 3.3313\n",
      "Epoch 166/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5919 - val_loss: 3.3335\n",
      "Epoch 167/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5897 - val_loss: 3.3225\n",
      "Epoch 168/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5878 - val_loss: 3.3510\n",
      "Epoch 169/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5895 - val_loss: 3.3544\n",
      "Epoch 170/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5891 - val_loss: 3.3835\n",
      "Epoch 171/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5858 - val_loss: 3.3447\n",
      "Epoch 172/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5851 - val_loss: 3.3846\n",
      "Epoch 173/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5868 - val_loss: 3.3502\n",
      "Epoch 174/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5878 - val_loss: 3.3561\n",
      "Epoch 175/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5863 - val_loss: 3.3649\n",
      "Epoch 176/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5882 - val_loss: 3.3485\n",
      "Epoch 177/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5818 - val_loss: 3.3781\n",
      "Epoch 178/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5840 - val_loss: 3.3845\n",
      "Epoch 179/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5826 - val_loss: 3.4057\n",
      "Epoch 180/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5799 - val_loss: 3.3977\n",
      "Epoch 181/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5768 - val_loss: 3.4183\n",
      "Epoch 182/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5774 - val_loss: 3.3980\n",
      "Epoch 183/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5779 - val_loss: 3.3918\n",
      "Epoch 184/200\n",
      "78584/78584 [==============================] - 7s 90us/step - loss: 0.5768 - val_loss: 3.3958\n",
      "Epoch 185/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5751 - val_loss: 3.4183\n",
      "Epoch 186/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5780 - val_loss: 3.4232\n",
      "Epoch 187/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5740 - val_loss: 3.3845\n",
      "Epoch 188/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5747 - val_loss: 3.4202\n",
      "Epoch 189/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5745 - val_loss: 3.4049\n",
      "Epoch 190/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5752 - val_loss: 3.4451\n",
      "Epoch 191/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5732 - val_loss: 3.4468\n",
      "Epoch 192/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5726 - val_loss: 3.4202\n",
      "Epoch 193/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5703 - val_loss: 3.4243\n",
      "Epoch 194/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5709 - val_loss: 3.4232\n",
      "Epoch 195/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5679 - val_loss: 3.4487\n",
      "Epoch 196/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5714 - val_loss: 3.4534\n",
      "Epoch 197/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5679 - val_loss: 3.4275\n",
      "Epoch 198/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5694 - val_loss: 3.4584\n",
      "Epoch 199/200\n",
      "78584/78584 [==============================] - 7s 88us/step - loss: 0.5677 - val_loss: 3.4476\n",
      "Epoch 200/200\n",
      "78584/78584 [==============================] - 7s 89us/step - loss: 0.5676 - val_loss: 3.4608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f438427d630>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bmodel.fit(X, Y, epochs=200, validation_split=0.1, batch_size=128, initial_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel.fit(X, Y, epochs=250, validation_split=0.1, batch_size=256, initial_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 40, 256)           303104    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 256)               526336    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                9766      \n",
      "=================================================================\n",
      "Total params: 839,206\n",
      "Trainable params: 839,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 88909 samples, validate on 4680 samples\n",
      "Epoch 1/80\n",
      "88909/88909 [==============================] - 67s 757us/step - loss: 2.6870 - acc: 0.2453 - val_loss: 2.3504 - val_acc: 0.3327\n",
      "Epoch 2/80\n",
      "88909/88909 [==============================] - 67s 751us/step - loss: 2.2018 - acc: 0.3632 - val_loss: 2.1132 - val_acc: 0.3771\n",
      "Epoch 3/80\n",
      "88909/88909 [==============================] - 67s 751us/step - loss: 2.0213 - acc: 0.4063 - val_loss: 1.9693 - val_acc: 0.4150\n",
      "Epoch 4/80\n",
      "88909/88909 [==============================] - 67s 752us/step - loss: 1.9042 - acc: 0.4368 - val_loss: 1.8828 - val_acc: 0.4387\n",
      "Epoch 5/80\n",
      "88909/88909 [==============================] - 67s 752us/step - loss: 1.8144 - acc: 0.4588 - val_loss: 1.8262 - val_acc: 0.4556\n",
      "Epoch 6/80\n",
      "35168/88909 [==========>...................] - ETA: 39s - loss: 1.7589 - acc: 0.4725"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(CuDNNLSTM(256, input_shape=(length, len(chars)), return_sequences=True))\n",
    "model2.add(CuDNNLSTM(256))\n",
    "model2.add(Lambda(lambda x: x / 1.))\n",
    "model2.add(Dense(len(chars), activation='softmax'))\n",
    "opt2 = optimizers.RMSprop(lr=0.0002)\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=opt2, metrics=[\"accuracy\"])\n",
    "model2.summary()\n",
    "history2 = model2.fit(X_full_shuff, Y_full_shuff,\n",
    "                  epochs=80,\n",
    "                  validation_split=0.05,\n",
    "                  callbacks=[callbacks.ModelCheckpoint('../models/alexcdot_cudnnlstm_2_256_layer_rms_0002_nostep_best_val_t_1.h5', save_best_only=True),\n",
    "                             callbacks.ModelCheckpoint('../models/alexcdot_cudnnlstm_2_256_layer_rms_0002_nostep_best_loss_t_1.h5', monitor=\"loss\", save_best_only=True)])\n",
    "model2.save('../models/alexcdot_cudnnlstm_2_512_layer_rms_0002_nostep_40_epochs_lr_t_1.h5')\n",
    "\n",
    "print(\"Done T=1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "\n",
    "def sample(m, seed):\n",
    "    # print(seed)\n",
    "    x_pred = np.zeros((1, length, len(chars)))\n",
    "    for t, char in enumerate(seed):\n",
    "        x_pred[0, t, char_index[char]] = 1\n",
    "    probs = m.predict(x_pred)[0]\n",
    "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
    "\n",
    "def create_sonnet(seed, n_lines, m):\n",
    "    sonnet = \"\"\n",
    "    curr_seed = seed\n",
    "    while n_lines >= 0:\n",
    "        next_ind = sample(m, curr_seed)\n",
    "        next_char = index_char[next_ind]\n",
    "        curr_seed = seed[1:] + next_char\n",
    "        sonnet += next_char\n",
    "        if next_char == \"\\n\":\n",
    "            n_lines -= 1\n",
    "            print(n_lines)\n",
    "    return sonnet\n",
    "\n",
    "def create_sonnet_fixed_lines(seed, n_lines, m):\n",
    "    sonnet = []\n",
    "    curr_seed = seed\n",
    "    for i in tqdm_notebook(range(n_lines)):\n",
    "        curr_line = \"\"\n",
    "        for j in range(len(seed)-1):\n",
    "            next_ind = sample(m, curr_seed)\n",
    "            next_char = index_char[next_ind]\n",
    "            # while next_char == \"\\n\":\n",
    "            #    print(probs)\n",
    "            #    next_ind = sample(m, curr_seed)\n",
    "            #    next_char = index_char[next_ind]\n",
    "            curr_seed = curr_seed[1:] + next_char\n",
    "            curr_line += next_char\n",
    "        # Artifically induce next line\n",
    "        curr_seed = curr_seed[1:] + \"\\n\"\n",
    "        sonnet.append(curr_line)\n",
    "    return sonnet\n",
    "\n",
    "\n",
    "def create_sonnet_no_lines(seed, n_lines, m):\n",
    "    sonnet = seed\n",
    "    curr_seed = seed\n",
    "    for i in range((n_lines-1) * len(seed)):\n",
    "        next_ind = sample(m, curr_seed)\n",
    "        next_char = index_char[next_ind]\n",
    "        curr_seed = seed[1:] + next_char\n",
    "        sonnet += next_char\n",
    "    return sonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-43d2bb17b941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmain_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"shall i compare thee to a summer's day?\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_sonnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(create_sonnet_no_lines(main_seed, 14, temperature=5.0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1f569f006854>\u001b[0m in \u001b[0;36mcreate_sonnet\u001b[0;34m(seed, n_lines, m)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcurr_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mn_lines\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mnext_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcurr_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1f569f006854>\u001b[0m in \u001b[0;36msample\u001b[0;34m(m, seed)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# print(seed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "main_seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "print(create_sonnet(main_seed, 14, 5))\n",
    "# print(create_sonnet_no_lines(main_seed, 14, temperature=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
